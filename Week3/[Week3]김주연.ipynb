{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[Week3] 텍스트분석 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 텍스트 분석 기본 용어 정리\n",
    "https://wikidocs.net/21694  \n",
    "위의 링크와 구글링으로 정리  \n",
    "용어 정리는 추후에 본인이 보기에 편한 형식으로 정리하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 토큰화 (Tokenization)  \n",
    "\\[답변] 주어진 문장, corpus를 token이라는 단위로 나누는 작업. token은 상황에 따라 기준이 달라진다. word, sentence, 등등. 한국어의 경우 형태소가 중요하다. NLTK는 영어 corpus tokenization에 사용한다. 한국어는 KoNLPY를 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 불용어 (Stopword)  \n",
    "\\[답변] 큰 의미가 없는 token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 정규표현식 (Regular Expression) 문법  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 정규 표현식에 해당하는 스트링 2개 이상  \n",
    "(1) a+  \n",
    "\\[답변] a, aa, aaaaaaaaaaaaa\n",
    "<br>\n",
    "(2) a{3}b{2,}  \n",
    "\\[답변] aaabb, aaabbbbbbbbbbbbbbbbbbbb\n",
    "<br>\n",
    "(3) .+b$  \n",
    "\\[답변] kkb, nb, dfb\n",
    "<br>\n",
    "(4) ^ab*     \n",
    "\\[답변] a, ab, abdflskejf\n",
    "<br>\n",
    "(5) \\[a-z]+\\[^a|z]  \n",
    "\\[답변] cb, cdb, xsz, xssafaefz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='xssafaef'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r = re.compile(\"[a-z]+[^a|z]\")\n",
    "r.search(\"xssafaefz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 정규 표현식 문자 규칙 의미  \n",
    "(1) \\\\\\ \\[답변] \\문자\n",
    "<br>\n",
    "(2) \\d \\[답변] 모든 숫자. [0-9]\n",
    "<br>\n",
    "(3) \\D \\[답변] 숫자 제외 모든 문자. [^0-9]\n",
    "<br>\n",
    "(4) \\s \\[답변] 공백. [ \\t\\n\\r\\f\\v]\n",
    "<br>\n",
    "(5) \\S \\[답변] 공백 제외 모든 문자. [^ \\t\\n\\r\\f\\v]\n",
    "<br>\n",
    "(6) \\w \\[답변] 문자 또는 숫자. [a-zA-Z0-9]\n",
    "<br>\n",
    "(7) \\W \\[답변] (문자 또는 숫자) 제외 문자. [^a-zA-Z0-9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 정규표현식 모듈 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 출력 결과 : \\['010', '1234', '1234', '30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '5678', '25']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"이름 : 눈송이\n",
    "전화번호 : 010-1234-5678\n",
    "나이 : 25\n",
    "성별 : 여\"\"\" \n",
    "re.findall(\"\\d+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 출력 결과 : \\['1234', '5678']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1234', '5678']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"이름 : 눈송이\n",
    "전화번호 : 010-1234-5678\n",
    "나이 : 25\n",
    "성별 : 여\"\"\"  \n",
    "re.findall(\"\\d{4}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 출력 결과 : \\['John', 'James', 'Noonsong'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Noonsong']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Noonsong   STUD\"\"\" \n",
    "re.findall(\"[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 정수 인코딩(Integer Encoding)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 문장 분리 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 셀은 실행만 시키면 됩니다.\n",
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "# 문장 분리\n",
    "sentences = text.replace(\"!\", \".\").split(\". \")\n",
    "# 토큰화\n",
    "tokenized = [sentence.lower().split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A barber is a person',\n",
       " 'a barber is good person',\n",
       " 'a barber is huge person',\n",
       " 'he Knew A Secret',\n",
       " 'The Secret He Kept is huge secret',\n",
       " 'Huge secret',\n",
       " 'His barber kept his word',\n",
       " 'a barber kept his word',\n",
       " 'His barber kept his secret',\n",
       " 'But keeping and keeping such a huge secret to himself was driving the barber crazy',\n",
       " 'the barber went up a huge mountain.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'barber', 'is', 'a', 'person'],\n",
       " ['a', 'barber', 'is', 'good', 'person'],\n",
       " ['a', 'barber', 'is', 'huge', 'person'],\n",
       " ['he', 'knew', 'a', 'secret'],\n",
       " ['the', 'secret', 'he', 'kept', 'is', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['his', 'barber', 'kept', 'his', 'word'],\n",
       " ['a', 'barber', 'kept', 'his', 'word'],\n",
       " ['his', 'barber', 'kept', 'his', 'secret'],\n",
       " ['but',\n",
       "  'keeping',\n",
       "  'and',\n",
       "  'keeping',\n",
       "  'such',\n",
       "  'a',\n",
       "  'huge',\n",
       "  'secret',\n",
       "  'to',\n",
       "  'himself',\n",
       "  'was',\n",
       "  'driving',\n",
       "  'the',\n",
       "  'barber',\n",
       "  'crazy'],\n",
       " ['the', 'barber', 'went', 'up', 'a', 'huge', 'mountain.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 각 단어에 대한 빈도수 딕셔너리 저장  \n",
    "\\[출력] \\[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain.', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usersilsoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain.', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 각 단어에 대한 빈도수 딕셔너리로 저장 \n",
    "# *stopword는 제거\n",
    "from nltk.corpus import stopwords\n",
    "vocab = {}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentences = []      # stopword 제거된 토큰화된 문장 리스트 저장\n",
    "# 여기부터 코드 작성\n",
    "for sentence in tokenized:\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        if word not in stop_words:\n",
    "            result.append(word)\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0\n",
    "            vocab[word] += 1\n",
    "    sentences.append(result)\n",
    "    \n",
    "# 빈도수 높은 순으로 정렬\n",
    "# 여기부터 코드 작성\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 빈도순으로 정렬된 단어에 정수 인덱스 부여  \n",
    "\\[출력] {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13, 'OOV' : 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain.': 13, 'OOV': 14}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 1\n",
    "# 여기부터 코드 작성\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    word_to_index[word] = i\n",
    "    i += 1\n",
    "# dictionary에 없는 단어\n",
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩  \n",
    "\\[출력] \\[\\[1, 5], \\[1, 8, 5], \\[1, 3, 5], \\[9, 2], \\[2, 4, 3, 2], \\[3, 2], \\[1, 4, 6], \\[1, 4, 6], \\[1, 4, 2], \\[7, 7, 3, 2, 10, 1, 11], \\[1, 12, 3, 13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "# 여기부터 코드 작성\n",
    "for sentence in sentences:\n",
    "    temp = []\n",
    "    for token in sentence:\n",
    "        try:\n",
    "            temp.append(word_to_index[token])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 패딩(Padding)  \n",
    "0으로 패딩해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 가장 긴 문장 길이 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(item) for item in encoded) # 가장 긴 문장 길이 찾기\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 가장 긴 문장 길이에 맞춰 0으로 채워넣기  \n",
    "\\[\\[ 1  5  0  0  0  0  0]  \n",
    " \\[ 1  8  5  0  0  0  0]  \n",
    " \\[ 1  3  5  0  0  0  0]  \n",
    " \\[ 9  2  0  0  0  0  0]  \n",
    " \\[ 2  4  3  2  0  0  0]  \n",
    " \\[ 3  2  0  0  0  0  0]  \n",
    " \\[ 1  4  6  0  0  0  0]  \n",
    " \\[ 1  4  6  0  0  0  0]  \n",
    " \\[ 1  4  2  0  0  0  0]  \n",
    " \\[ 7  7  3  2 10  1 11]  \n",
    " \\[ 1 12  3 13  0  0  0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  0  0  0  0  0]\n",
      " [ 1  8  5  0  0  0  0]\n",
      " [ 1  3  5  0  0  0  0]\n",
      " [ 9  2  0  0  0  0  0]\n",
      " [ 2  4  3  2  0  0  0]\n",
      " [ 3  2  0  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  6  0  0  0  0]\n",
      " [ 1  4  2  0  0  0  0]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for sentence in encoded:\n",
    "    # 코드 작성\n",
    "    while len(sentence) < max_len:\n",
    "        sentence.append(0)\n",
    "\n",
    "encoded = np.array(encoded)        # 지금은 보기 좋게 하기 위함임..\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 원-핫 인코딩(One-Hot Encoding)  \n",
    "\\[출력] \\[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word_to_index):\n",
    "    # 코드 작성\n",
    "    one_hot_vector = [0] * len(word_to_index)\n",
    "    index = word_to_index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 함수와 정수 인코딩 실행 후 실행할 것.\n",
    "one_hot_encoding(\"huge\", word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Bag of Words(BoW)  \n",
    "\\[답변] 단어 출현 frequency를 저장한 단어 가방. 각 단어에 정수 인덱스를 부여하고 각 인덱스 위치에 token 등장 횟수를 기록한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 분석 전처리 경험해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 본인이 좋아하는 노래 가사, 시 등 text에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = # 좋아하는 문장들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 사용해보셔도 됩니다.\n",
    "# Tokenizer 사용 시 시간이 좀 걸리기 때문에 그냥 띄어쓰기 기준으로 나눴습니다,\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = # 본인이 생각하는 불용어 리스트 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 빈도수 리스트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "# 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 워드 클라우드 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# word_dict 를 이용해 wordcloud 만들기\n",
    "# 코드 작성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
