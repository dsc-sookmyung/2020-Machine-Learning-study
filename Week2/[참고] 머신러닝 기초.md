
# 1. 머신러닝 기본 용어

## 0. 머신러닝이란?

컴퓨터가 명시적으로 프로그램 되지 않고도 학습할 수 있도록 하는 연구 분야

- 지도학습 : 입력과 결과값(label) 이용한 학습 (분류 classification, 회귀 regression)
- 비지도학습 : 입력만을 이용한 학습 (군집화 clustering, 압축 compression)
- 강화학습 : 결과값 대신 보상(reward) 주워짐 (Action Selection, Policy Learning)

## 1. Overfitting & Underfitting

- **Overfitting** : training할 때 사용된 dataset만 예측을 잘하지만, 새로운 데이터들은 예측하지 못함.
    - overfitting 막는 방법 : bias 높이고 variance 낮추기, Early stopping, Drop-out, 모델 단순화
- **Underfitting** : training dataset조차 예측을 잘 못함.
    - underfitting 막는 방법 : bias를 낮추고 variance를 높이기.

![http://sanghyukchun.github.io/images/post/59-1.png](http://sanghyukchun.github.io/images/post/59-1.png)

## 2. Norm

벡터의 길이 혹은 크기를 측정하는 방법(함수)

- L1 Norm

  L1 = \Sigma(x_i) = |x_1| + |x_2| + ... + |x_n|

- L2 Norm : KNN, K-means

  L2 = \sqrt{\Sigma(x_i)^2} = \sqrt{x_1^2 + x_2^2 + ... + x_n^2}

## 3. PCA 차원 축소

여러 변수 간 존재하는 상관관계를 이용해 이를 대표하는 주성분을 추출해 차원을 축소하는 기법

[PCA(차원 축소)란](https://medium.com/@john_analyst/pca-%EC%B0%A8%EC%9B%90-%EC%B6%95%EC%86%8C-%EB%9E%80-3339aed5afa1)

## 4. Confusion Matrix (혼동 행렬)

- Confusion matrix : 분류 모델의 성능 평가에 사용되는 표
- **TP, TN, FP, FN** 개념 잘 알아 둘 것!

![https://user-images.githubusercontent.com/45345120/97946467-e1479c80-1dcd-11eb-9a68-4c87189535ef.png](https://user-images.githubusercontent.com/45345120/97946467-e1479c80-1dcd-11eb-9a68-4c87189535ef.png)

- 3가지 평가 척도
    - **Accuracy(정확도)** : (TP + TN) / (TP + TN + FP + FN)
    - **Precision(정밀도)** : TP / (TP + FP)
    모델이 **True라고 분류한 것 중**에서 실제 True인 것의 비율
    - **Recall(재현율)** : TP / (TP + FN)
    **실제 True인 것 중**에서 모델이 True라고 예측한 것의 비율
    hit rate로도 사용됨
- Precision은 모델 입장, Recall은 실제 정답 입장
- Precision과 Recall은 상호보완적으로 사용될 수 있음. 두 지표가 모두 높을수록 좋은 모델.

```python
# Heatmap 그리기
import seaborn as sns
...
sns.heatmap('''dataframe 변수명''', annot = True)
...
```

### 번외 ) ROC curve

![https://user-images.githubusercontent.com/45345120/97947056-7303d980-1dcf-11eb-931c-edcbf7a59813.png](https://user-images.githubusercontent.com/45345120/97947056-7303d980-1dcf-11eb-931c-edcbf7a59813.png)

x축 : 실제 False인 data 중에서 모델이 True로 분류(Fallout)

y축 : 실제 True인 data 중에서 모델이 True로 분류한 비율 (Recall)

- curve가 **왼쪽 위 모서리에 가까울수록** 모델의 성능이 좋다고 평가
- AUC(Area Under Curve) :ROC curve 그래프 아래의 면적값을 이용. 최대값 = 1
⇒ 좋은 모델일수록 1에 가까운 값

[참고] [https://bioinformaticsandme.tistory.com/328](https://bioinformaticsandme.tistory.com/328)

---

# 2. 머신러닝 알고리즘

## 1. KNN (k-Nearest Neighbors)

새로운 데이터를 받았을 때, **가까운 거리에 있는 K개**의 라벨을 함께 본다 (분류모델)

![image](https://user-images.githubusercontent.com/35582991/103339037-ab6c1f00-4ac3-11eb-8daf-c09e4c415dd7.png)


```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
# iris data 로드
iris = load_iris()
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=0)
# classifier 객체 생성
knn = KNeighborsClassifier(n_neighbors=1, n_jobs=-1)
knn.fit(x_train, y_train)

prediction = knn.predict(x_test)
results = [iris.target_names[p] for p in prediction]
print('results의 처음 6개 ==> {}'.format(results[:6]))

# 정확도 계산
print('score 메소드 이용 ==> {:.3f}'.format(knn.score(x_test, y_test)))
```

## 2. Decision Tree

전형적인 분류 모델, 매우 직관적. 시각적으로 읽히기 쉬운 형태로 나타남(분류 모델)

![image](https://user-images.githubusercontent.com/35582991/103339047-b030d300-4ac3-11eb-92df-68020e255cba.png)

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_text

decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)
decision_tree = decision_tree.fit(x_train, y_train)
r = export_text(decision_tree, feature_names=iris['feature_names'])
print(r)

prediction = decision_tree.predict(x_test)
```

## 3. Naive Bayes

### 3-1. 베이즈 정리

  P(A|B) = \frac{P(A \cap B)}{P(B)}

### 3-2. 나이브 베이즈(Naive Bayes)

* 데이터셋의 모든 특징들이 동등하고 독립적이라고 가정

  P(B|A) = \frac{P(A|B)P(B)}{P(A)}

ex) 스팸메일 분류하는 서비스에서 많이 사용

```python
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb = gnb.fit(x_train, y_train)

prediction = gnb.predict(x_test)
```

## 4. SVM (Support Vector Machine)

비확률적 이진 선형 분류 모델

분류할 수 있는 여러 선들 중 **margin을 최대화**하는 선을 찾는다.

![image](https://user-images.githubusercontent.com/35582991/103339055-b8890e00-4ac3-11eb-8cb7-e69192d25768.png)

```python
from sklearn.svm import SVC

svc = SVC()
svc = svc.fit(x_train, y_train)

prediction = svc.predict(x_test)
```

## 5. Linear Regression

[https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html)

ex) 학생 점수 예측

  y=β_0+β_1x_1+β_2x_2+...+β_nx_n

```python
from sklearn.svm import SVC

svc = SVC()
svc = svc.fit(x_train, y_train)

prediction = svc.predict(x_test)
```

## 6. Logistic Regression

ex) Pass / Fail 예측 (분류 모델)

- Linear Regression의 문제점 : 확률을 나타낼 수 없음(확률 범위 0~1)
- Logistic Regression을 이용하게 되면 0~1사이의 값을 갖는 확률로 표현이 가능

![image](https://user-images.githubusercontent.com/35582991/103339063-bcb52b80-4ac3-11eb-971c-72af2d8e52ce.png)

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr = lr.fit(x_train, y_train)

prediction = lr.predict(x_test)
```

## 7. Random Forest

Decision tree의 한 종류. 각각의 트리가 개별적으로 종류 추정 —> Vote

모든 트리의 각 선택을 고려해 가장 많이 선택된 분류를 고름

![image](https://user-images.githubusercontent.com/35582991/103339066-bfb01c00-4ac3-11eb-8d1b-eacda8443f2d.png)

```python
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(max_depth=2, random_state=0)
clf = clf.fit(x_train, y_train)

prediction = clf.predict(x_test)
```

## 8. Ensemble

지도학습의 대표적 기법 중 하나.

여러 모형을 기반으로 예측을 결합하여 최종 결과를 내는 모형.

- Bagging : 입력 데이터를 모델 수 만큼 나눈 뒤, 각각 학습시킴. (병렬적인 기법)

![image](https://user-images.githubusercontent.com/35582991/103339074-c343a300-4ac3-11eb-90b6-ad2c22a52997.png)

- Boosting : 하나의 모델을 만들어 Error 계산. Error에 기여한 Sample마다 다른 가중치를 주고 해당 error를 감소시키는 새로운 모델 학습. (순차적인 기법)

![image](https://user-images.githubusercontent.com/35582991/103339085-c9d21a80-4ac3-11eb-8d50-be0cfa41f88e.png)

## 9. K-means clustering

비지도학습 알고리즘

1. k개의 중심점(centroid)들을 선택
2. 각 데이터가 가장 가까운 중심점과 결합하여 클러스터를 만듦
3. 결합한 후 각 클러스터 내 새로운 중심 생성
4. 새로운 중심에 가까운 데이터 중심점이 다시 결합하여 클러스터 확장
5. **중심점이 바뀌지 않을 때까지 반복**

```python
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.DataFrame(array, columns=['x', 'y'])

# convert dataframe to numpy array
data_points = df.values
kmeans =  KMeans(n_clusters = 3).fit(data_points) # cluster 개수 = 3 으로 KMeans clustering
df['cluster_id'] = kmeans.labels_  # clustering된 각 점의 cluster 번호 저장

# Visualize data point
sns.lmplot('x', 'y', data=df, fit_reg=False,                    
            scatter_kws={"s": 200},
            hue="cluster_id")
plt.title("kmean plot")
plt.xlabel("x")
plt.ylabel("y")
```

![image](https://user-images.githubusercontent.com/35582991/103339094-cf2f6500-4ac3-11eb-86be-e025b1f04bab.png)

