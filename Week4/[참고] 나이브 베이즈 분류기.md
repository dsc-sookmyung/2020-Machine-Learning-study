# 1. Naive Bayes classification

### 1. Bayes' theorem

P(A) 가 A가 일어날 확률, P(B)가 B가 일어날 확률, P(B|A)가 A가 일어나고나서 B가 일어날 확률, P(A|B)가 B가 일어나고나서 A가 일어날 확률

  P(A|B) = \frac{P(A \cap B)}{P(B)}

### 2. Naive Bayes Classification

**P(정상 메일 | 입력 테스트) = 입력 텍스트가 있을 때 정상 메일일 확률**

**P(스팸 메일 | 입력 테스트) = 입력 텍스트가 있을 때 스팸 메일일 확률**

베이즈의 정리에 따라서 식을 표현

**P(정상 메일 | 입력 테스트) = (P(입력 테스트 | 정상 메일) × P(정상 메일)) / P(입력 텍스트)**

**P(스팸 메일 | 입력 테스트) = (P(입력 테스트 | 스팸 메일) × P(스팸 메일)) / P(입력 텍스트)**

나이브 베이즈 분류기는 모든 단어가 독립적이라고 가정! (본문에 있는 단어 3개 w1, w2, w3)

**P(입력 테스트 | 정상 메일)** **= P(w1 | 정상 메일) × P(w2 | 정상 메일) × P(w3 | 정상 메일) × P(정상 메일)**

**P(입력 테스트 | 스팸 메일) = P(w1 | 스팸 메일) × P(w2 | 스팸 메일) × P(w3 | 스팸 메일) × P(스팸 메일)**

* 오직 빈도수만을 고려

# 2. 나이브베이즈 분류기를 이용해 긍정/부정 분류

[03 Naive Bayes Classifier - YBIGTA Data Science](https://ybigta-data-science.readthedocs.io/en/latest/6_Data_Science_from_Scratch/03_Naive%20Bayes%20Classifier/)

### 1. 데이터 불러오기

```python
# pandas 이용해 csv를 dataframe으로 읽어오기
df = pd.read_csv("./train.csv")
# "text", "target" column만 갖고오기
df = df[["text", "target"]]
df.head()
```

### 2. target 값 기준(0/1)별로 text 리스트에 저장

```python
# target == 0
df_zero = []
# target == 1
df_one = []

for i in range(len(df)):
    if df.iloc[i,1] == 0:
        df_zero.append(df.iloc[i,0])
    else:
        df_one.append(df.iloc[i,0])
```

### 3. train data에 저장

```python
train_datas = [" ".join(df_zero), " ".join(df_one)]
```

### 4. 데이터 전처리

```python
def preprocessing(data):
    # 스탑워드 제거
    data_sw = re.compile('[\W]').sub(' ', data.lower())
    # 토큰화
    tokenized = data_sw.split()
    # Bow화 {단어 : 빈도수 형태}
    word_vector = dict(Counter(tokenized))
    return word_vector, len(tokenized)

train_zero, zero_total_wc = preprocessing(train_datas[0])
train_one, one_total_wc = preprocessing(train_datas[1])
```

### 5. 문장이 나올 확률 구하기 (log 이용)

```python
nowords_w = 0.1
# 문장이 나올 확률 = 각 단어가 등장할 확률 다 곱한 것
# log(P(test_data|긍정(train_data)))
# log(P(test_data|부정(train_data)))
def getLogProb(train_vector, test_vector, total_wc):
    log_prob = 0
    for word in test_vector:
        # word가 train_vector에 있는 경우 --> 단어 등장 확률 // 없는 경우 --> 사전에 임의로 정한 확률
        if word in train_vector:
            log_prob += log(train_vector[word]/total_wc)
        else:
            # 없는 단어는 가정
            log_prob += log(nowords_w/total_wc)
    return log_prob

test_data = "Traffic accident"        # 예측 대상 문장 #input()으로 대체 가능
test_vector, _ = preprocessing(test_data)
```

### 6. P(target == 0), P(target == 1) 구하기

```python
zero_prob = zero_total_wc / (zero_total_wc + one_total_wc) # P(target == 0)
one_prob = one_total_wc / (zero_total_wc + one_total_wc)  # P(target == 1)
```

### 7. 베이즈 정리 이용 - 로그연산

```python
# P(test_data) 는 공통이므로 신경쓰지 않음
# P(부정|test_data) = P(test_data|부정) * P(부정) / P(test_data)
test_zero_prob = getLogProb(train_zero, test_vector, zero_total_wc) + log(zero_prob) 
# P(긍정|test_data) = P(test_data|긍정) * P(긍정) / P(test_data)
test_one_prob = getLogProb(train_one, test_vector, one_total_wc) + log(one_prob)
```

### 8. 로그를 지수화, Normalize

```python
# test_one_prob, test_zero_prob 중 큰 것을 찾아 똑같이 빼주어 하나가 0이 되도록
maxprob = max(test_one_prob, test_zero_prob)
test_one_prob -= maxprob
test_zero_prob -= maxprob

# log 없애주기 위해 지수화 --> 힌트) exp 함수 사용
test_one = exp(test_one_prob)
test_zero = exp(test_zero_prob)

# 두 확률 값의 상대적인 비율 구하기(normalization)
normalized_prob = [test_zero/(test_one+test_zero), test_one/(test_one+test_zero)]
# 둘 중 큰 것의 index 출력
normalized_prob.index(max(normalized_prob))
```
